# Visual-Question-Answering

# ğŸ¤– Visual Question Answering (VQA) using CLIP + ResNet Framework

This repository contains the implementation and results of our VQA research:

> **Bridging Vision and Language: A CLIP-ResNet Framework for Visual Question Answering**  
> ğŸ“ Authors: Rupa Kandula, Jishnu Teja Dandamudi, Rama Muni Reddy Yanamala  
> ğŸ“ Affiliation: Amrita Vishwa Vidyapeetham, Coimbatore, India  
> ğŸ—“ï¸ Year: 2025  
> ğŸ“„ Dataset: DAQUAR (publicly available)

---

## ğŸ” Abstract

Visual Question Answering (VQA) is a multimodal AI task that requires understanding both images and natural language to answer visual questions. In this project, we propose a **hybrid model** that fuses:

- **Visual features** from a pre-trained **ResNet-18**
- **Textual embeddings** from **CLIP (Contrastive Language-Image Pretraining)**

These features are concatenated and passed through a classification layer to predict the correct answer from a predefined set.

---

## ğŸ§  Model Architecture

- ğŸ”· **Image Encoder**: ResNet-18 (output: 512-dim vector)
- ğŸ”¶ **Text Encoder**: CLIP tokenizer and text transformer (output: 512-dim vector)
- ğŸ”— **Fusion Layer**: Concatenation â†’ 1024-dim joint vector
- ğŸ¯ **Classifier**: Fully Connected â†’ Softmax â†’ Answer Prediction

### ğŸ“Š Training Configuration

- Loss Function: CrossEntropyLoss
- Optimizer: Adam
- Evaluation Metrics: Accuracy, Weighted Precision
- Best Accuracy Achieved: **95.4%** on DAQUAR dataset

---


